# -*- coding: utf-8 -*-
"""greenhouse_env.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1syqtvwawXwXs_-ebs-ltcLLU1YgUzfXl

Import libraries
"""

import pickle
import numpy as np
import random
from scipy import odr
import torch
import torch.nn as nn
import torch.nn.functional as F
import gymnasium as gym
from gymnasium.spaces import Box

gym.logger.set_level(40)
torch.set_num_threads(1)
np.warnings.filterwarnings('ignore', category=np.VisibleDeprecationWarning)

"""Network"""

class Net(nn.Module):
    def __init__(self, input_dim, output_dim, hidden_dim=120):
        super(Net, self).__init__()
        self.fc1 = torch.nn.Linear(input_dim, hidden_dim)
        self.fc2 = torch.nn.Linear(hidden_dim, hidden_dim)
        self.fc3 = torch.nn.Linear(hidden_dim, hidden_dim)
        self.fc4 = torch.nn.Linear(hidden_dim, hidden_dim)
        self.fc5 = torch.nn.Linear(hidden_dim, output_dim)

    def forward(self, x):
        if not isinstance(x, torch.Tensor):
            x = torch.tensor(x, dtype=torch.float)
        x = self.fc1(x)
        x = F.relu(x)
        x = self.fc2(x)
        x = F.relu(x)
        x = self.fc3(x)
        x = F.relu(x)
        x = self.fc4(x)
        x = F.relu(x)
        x = self.fc5(x)
        return x

"""Reward function"""

class Economic:
    def __init__(self, dayAction, dayState, dayReward):
        self.dayAction = dayAction  # [heating temp, ventilation, lamps, scr_enrg, scr_blck, co2]
        self.dayState = dayState  # [AirT, AirRH, Airppm]
        self.dayReward = dayReward  # [FW]

    def cal_economic(self):

        gains = self.cal_gains()
        elecCost = self.elec_cost()
        co2Cost = self.co2_cost()
        heatCost = self.heat_cost()

        variableCosts = elecCost + co2Cost + heatCost
        balance = gains - variableCosts
        # balance = gains

        economic = {'balance': balance,
                    'gains': gains,
                    'variableCosts': variableCosts,
                    'elecCost': elecCost,
                    'co2Cost': co2Cost,
                    'heatCost': heatCost}

        return economic

    def cal_gains(self):
        # price = 3.5
        price = 3.185
        # price = 3.1
        # price = 2.94
        return self.dayReward[0] * price

    def elec_cost(self):
        lmp_use = self.dayAction[:, 2]
        days = len(lmp_use) // 24

        power = 185 / 2.1

        price = np.array(([0.04] * 7 + [0.08] * 16 + [0.04] * 1) * days)
        cost = np.sum(np.array(lmp_use) * price * power / 1000)

        return cost

    def co2_cost(self):
        CO2_setpoint = self.dayAction[:, 5]
        Airppm = self.dayState[:, 2]

        McConAir_max = 4e-6
        co2_use = CO2_setpoint - Airppm
        co2_use[co2_use > 0] = McConAir_max
        co2_use[co2_use <= 0] = 0

        price1, price2 = 0.08, 0.2
        kgCO2 = sum(co2_use) * 3600
        firstTranche = min(kgCO2, 12)
        secondTranche = kgCO2 - firstTranche
        cost = firstTranche * price1 + secondTranche * price2

        return cost

    def heat_cost(self):
        temp_setpoint = self.dayAction[:, 0]
        AirT = self.dayState[:, 0]

        PConPipe1_max = np.full((24), 60)
        heat_use = (PConPipe1_max - AirT) * 2.1

        heat = temp_setpoint - AirT
        heat_use[heat <= 0] = 0

        price = 0.03
        cost = sum(heat_use) * price / 1000

        return cost

"""Environment"""

class GreenhouseEnv(gym.Env):

    def __init__(self, config):
        self.net1 = Net(14, 3, 300)
        self.net1.load_state_dict(torch.load(
            config['model1_dir'], map_location=torch.device('cpu')))
        self.net2 = Net(5, 1, 300)
        self.net2.load_state_dict(torch.load(
            config['model2_dir'], map_location=torch.device('cpu')))
        self.net3 = Net(2, 1, 600)
        self.net3.load_state_dict(torch.load(
            config['model3_dir'], map_location=torch.device('cpu')))
        self.net1.eval()
        self.net2.eval()
        self.net3.eval()

        self.scaler1_x = pickle.load(open(config['scaler1_x'], 'rb'))
        self.scaler1_y = pickle.load(open(config['scaler1_y'], 'rb'))
        self.scaler2_x = pickle.load(open(config['scaler2_x'], 'rb'))
        self.scaler2_y = pickle.load(open(config['scaler2_y'], 'rb'))
        self.scaler3_x = pickle.load(open(config['scaler3_x'], 'rb'))
        self.scaler3_y = pickle.load(open(config['scaler3_y'], 'rb'))

        self.linreg = pickle.load(open(config['linreg_dir'], 'rb'))

        self.full_weather = np.load(config['weather_dir'])

        self.observation_space = self.observation_space()
        self.action_space = self.action_space()

        self.reset()

        self._max_episode_steps = 48

    def observation_space(self):
        # AirT * 24, AirRH * 24, Airppm * 24; GR; FW
        low = np.concatenate(([0 for _ in range(24)], [0 for _ in range(24)],
                               [0 for _ in range(24)], [0, 0]))
        high = np.concatenate(([40 for _ in range(24)], [100 for _ in range(24)],
                                [6000 for _ in range(24)], [10, 300]))
        return Box(low=low, high=high, dtype=np.float32)

    def action_space(self):
        # heating temp, ventilation, lamps, scr_enrg, scr_blck, co2
        low = np.concatenate(
            ([0 for _ in range(24)], [0 for _ in range(24)], [0 for _ in range(24)], [0 for _ in range(24)],
             [0 for _ in range(24)], [0 for _ in range(24)]))
        high = np.concatenate(
            ([40 for _ in range(24)], [1 for _ in range(24)], [1 for _ in range(24)], [1 for _ in range(24)],
             [1 for _ in range(24)], [6000 for _ in range(24)]))

        return Box(low=low, high=high, dtype=np.float32)

    def f_co2(self, co2):
        beta_co2 = 400
        return 1 - np.exp(-co2 / beta_co2)

    def f_par(self, par):
        beta_par = 200
        return 1 - np.exp(-par / beta_par)

    def f_t(self, t):
        t_opt = 25
        t_diff = np.abs(t - t_opt)
        t_rng = 25
        if t_diff < t_rng:
            return 1 - (t_diff / t_rng) ** 2
        else:
            return 0

    def g(self, co2, par, t, fw, d, h):
        day_fw = fw[d*24:(d+1)*24]
        cum_gr_t = ((day_fw[h] + 1)/(day_fw[0] + 1)) ** (1/(h+1)) - 1
        hour_gr = 1.194 ** (1 / 12)
        max_gr = np.array([hour_gr, cum_gr_t])
        max_gr = np.min(max_gr)
        co2_r = self.f_co2(co2)
        par_r = self.f_par(par)
        t_r = self.f_t(t)
        return max_gr * co2_r * par_r * t_r

    def get_outside_weather(self, day_index):
        if day_index < 48:
            return self.full_weather[day_index * 24:(day_index + 1) * 24]
        else:
            mv_idx = day_index - 48
            return self.full_weather[(day_index - mv_idx) * 24:(day_index + 1-mv_idx) * 24]

    def calculate_reward(self, dayAction, dayState, dayReward):
        economic = Economic(dayAction=dayAction, dayState=dayState,
                            dayReward=dayReward).cal_economic()
        reward = economic['balance']

        return reward, economic

    def crop_weight(self, t):
        cm = 20
        rm = 0.14
        tb = 35.4
        w = cm/rm*np.log(1+np.exp(rm*(t-tb)))
        return w

    def poly_fw(self):
        days = 48
        d = np.array(range(days))

        days_fw = self.crop_weight(d)
        fw_model = odr.polynomial(5)
        FW = odr.Data(d, days_fw)
        odr_fw = odr.ODR(FW, fw_model)
        output = odr_fw.run()  # running ODR fitting
        poly = np.poly1d(output.beta[::-1])
        return poly(np.linspace(0, days, days * 24)).reshape(-1, 1)

    def _get_obs(self):
        observation = np.hstack((self.day_inside_weather, self.crop_state, self.fw))
        return observation

    def step(self, action):
        assert len(action) == 144, 'wrong input control dimension'

        # self.action = action

        action = action.reshape((6, 24)).T

        day_outside_weather = self.get_outside_weather(self.day_index)

        day_inside_weather = np.zeros((24, 3))

        cum_gr = 1  # cumulative growth rate

        days_fw = self.poly_fw()

        for i in range(24):
            # each hour
            # Tout, Rhout, Iglob, Windsp, PARout
            cur_outside_weather = day_outside_weather[i]
            cur_control = action[i]  # temp, vent, lamp, scr_enrg, scr_black, co2
            cur_inside_weather = self.inside_weather  # Tair, Rhair, CO2air
            day_inside_weather[i, :] = cur_inside_weather
            input1 = np.hstack(
                (cur_outside_weather, cur_control, cur_inside_weather))
            input1 = input1.reshape(1, -1)
            # input1 = input1.repeat(5,axis=0)
            input1_normal = self.scaler1_x.transform(input1)
            input1_normal = torch.tensor(input1_normal, dtype=torch.float)
            output1_normal = self.net1(input1_normal).detach().numpy()
            # output1_normal = output1_normal.reshape(1,-1)
            output1 = self.scaler1_y.inverse_transform(output1_normal)[0]
            output1 = np.clip(output1, [0, 0, 0], [40, 100, 6000])
            self.inside_weather = output1

            # PARsensor calculation
            PARsensor = self.linreg.predict(input1[0, [2, 4, 7]].reshape(1, -1))
            PARsensor = PARsensor if PARsensor > 50.0 else 0.0
            # PARsensor = np.array(PARsensor)
            # PARsensor = PARsensor.reshape(1,-1)

            # input2 = np.hstack(
            #     (self.inside_weather, PARsensor, self.crop_state))
            # # print(input2)
            # input2 = input2.reshape(1, -1)
            # input2_normal = self.scaler2_x.fit_transform(input2)
            # input2_normal = torch.tensor(input2_normal, dtype=torch.float)
            # output2_normal = self.net2(input2_normal).detach().numpy()
            # output2 = self.scaler2_y.inverse_transform(output2_normal)[0]

            # output2[-1] = np.maximum(self.crop_state[-1], output2[-1])

            # self.crop_state = output2  # LAI, PlantLoad, NetGrowth
            # print(self.inside_weather)
            # print(self.crop_state)

            gr = self.g(co2=output1[2],
                        par=PARsensor,
                        t=output1[0],
                        fw=days_fw,
                        d=self.day_index,
                        h=i)
            cum_gr *= (1 + gr)

        # input3 = np.concatenate((self.crop_state, self.fw))
        # # print(input3)
        # input3 = input3.reshape(1, -1)
        # input3_normal = self.scaler3_x.transform(input3)
        # input3_normal = torch.tensor(input3_normal, dtype=torch.float)
        # output3_normal = self.net3(input3_normal).detach().numpy()
        # output3 = self.scaler3_y.inverse_transform(output3_normal)[0]
        #
        # cur_fw = self.fw
        #
        # output3 = output3 if output3 > 0.1 else np.array([0])
        # output3 = np.maximum(output3, self.fw)
        # self.fw = output3
        # print('fw', self.fw)

        # day_fw = (self.fw - cur_fw) + self.store_fw
        # harvest = 1e-3
        # if day_fw < harvest:
        #     self.store_fw += self.fw - cur_fw
        #     day_fw = np.zeros(1)
        #
        # else:
        #     self.store_fw = np.zeros(1)

        cur_fw = self.fw.copy()
        self.fw *= cum_gr
        day_fw = self.fw - cur_fw

        reward, economic = self.calculate_reward(
            dayAction=action, dayState=day_inside_weather, dayReward=day_fw)

        self.day_inside_weather = day_inside_weather.T.reshape(1, -1)[0]
        obs = self._get_obs()

        self.day_index += 1

        done = self.day_index >= self._max_episode_steps

        return obs, reward, done, False, economic

    def reset(self, seed=None, options=None):
        random.seed(seed)

        self.inside_weather = np.array([17.27, 61.83, 737.31])
        self.day_inside_weather = np.concatenate((np.random.randint(20, 24, 24),
                                                  np.random.randint(50, 80, 24),
                                                  np.random.randint(600, 800, 24)))

        self.crop_state = np.zeros(1)

        self.fw = np.ones(1)

        self.store_fw = np.zeros(1)

        self.day_index = 0

        observation = self._get_obs()

        return observation, {}